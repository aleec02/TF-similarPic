{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nRONDA 3B - GUARDAR MODELO CLIP COMO .PTH (OPCIONAL)\n====================================================\n\nProp√≥sito:\n- Guardar el modelo CLIP completo como .pth para uso en producci√≥n\n- Eliminar dependencia de HuggingFace download en runtime\n- Acelerar startup de API (~30-60 segundos m√°s r√°pido)\n\n¬øPor qu√© guardar CLIP?\n- Startup m√°s r√°pido: 5-10s (local .pth) vs 30-60s (download HuggingFace)\n- Sin dependencia de internet en producci√≥n\n- Sin riesgo de downtime de HuggingFace\n- Embeddings est√°ticos en servidor\n\nTradeoff:\n- Tama√±o: +600 MB adicionales\n- Ventaja: M√°s confiable, m√°s r√°pido\n\nUso:\n  - Ejecutar DESPU√âS del script 06 (cuando CLIP ya est√° cargado)\n  - O ejecutar standalone para guardar CLIP\n\nInputs:\n  - Ninguno (descarga CLIP de HuggingFace)\n\nOutputs:\n  - clip_model.pth (~600 MB)\n  - clip_processor_config.json\n  - model_info.json\n\nAutor: similarPic Team\nFecha: 2025-11\n\"\"\"\n\nimport os\nimport sys\nfrom pathlib import Path\nimport time\nimport json\n\n# PyTorch\nimport torch\nimport torch.nn as nn\n\n# CLIP\nfrom transformers import CLIPModel, CLIPProcessor\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"=\" * 70)\nprint(\"  RONDA 3B - GUARDAR MODELO CLIP COMO .PTH\")\nprint(\"  Optimizaci√≥n para producci√≥n\")\nprint(\"=\" * 70)\n\n# ============================================\n# CONFIGURACI√ìN\n# ============================================\n\nIS_KAGGLE = os.path.exists('/kaggle/input')\nIS_COLAB = 'COLAB_GPU' in os.environ\n\nprint(f\"\\nüñ•Ô∏è Entorno: {'Kaggle' if IS_KAGGLE else 'Colab' if IS_COLAB else 'Local'}\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"üéÆ Device: {device}\")\n\n# Rutas\nif IS_KAGGLE:\n    output_dir = Path('/kaggle/working')\nelif IS_COLAB:\n    output_dir = Path('/content/fashion_processed')\nelse:\n    output_dir = Path('./models')\n\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nprint(f\"\\nüìÇ Output: {output_dir}\")\n\n# ============================================\n# 1. CARGAR CLIP DESDE HUGGINGFACE\n# ============================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"[1/3] CARGANDO CLIP DESDE HUGGINGFACE\")\nprint(\"=\" * 70)\n\nmodel_name = \"openai/clip-vit-base-patch32\"\nprint(f\"\\nüì• Descargando {model_name}...\")\n\nstart_time = time.time()\nclip_model = CLIPModel.from_pretrained(model_name)\nclip_processor = CLIPProcessor.from_pretrained(model_name)\ndownload_time = time.time() - start_time\n\nprint(f\"‚úÖ CLIP descargado en {download_time:.1f} segundos\")\nprint(f\"   - Modelo: ViT-B/32\")\nprint(f\"   - Par√°metros: {sum(p.numel() for p in clip_model.parameters()):,}\")\n\n# ============================================\n# 2. GUARDAR MODELO COMO .PTH\n# ============================================\n\nprint(\"\\n[2/3] GUARDANDO MODELO CLIP COMO .PTH\")\n\n# Guardar state_dict completo\nclip_path = output_dir / \"clip_model.pth\"\n\nprint(f\"\\nüíæ Guardando modelo...\")\nstart_time = time.time()\n\n# Opci√≥n 1: Guardar solo state_dict (m√°s ligero, recomendado)\ntorch.save({\n    'model_state_dict': clip_model.state_dict(),\n    'model_name': model_name,\n    'embedding_dim': 512,\n    'image_size': 224,\n}, clip_path)\n\nsave_time = time.time() - start_time\nfile_size_mb = clip_path.stat().st_size / (1024 ** 2)\n\nprint(f\"‚úÖ Modelo guardado: {clip_path}\")\nprint(f\"   - Tama√±o: {file_size_mb:.1f} MB\")\nprint(f\"   - Tiempo de guardado: {save_time:.1f} segundos\")\n\n# Guardar configuraci√≥n del processor\nprocessor_config_path = output_dir / \"clip_processor_config.json\"\nclip_processor.save_pretrained(str(output_dir / \"clip_processor\"))\n\nprint(f\"‚úÖ Processor config guardado: {processor_config_path.parent}\")\n\n# ============================================\n# 3. TEST DE CARGA (BENCHMARK)\n# ============================================\n\nprint(\"\\n[3/3] BENCHMARK: .PTH VS HUGGINGFACE\")\n\nprint(\"\\nüìä Test 1: Cargar desde .pth\")\nstart_time = time.time()\n\n# Recrear modelo\nclip_model_loaded = CLIPModel.from_pretrained(model_name)\ncheckpoint = torch.load(clip_path, map_location='cpu')\nclip_model_loaded.load_state_dict(checkpoint['model_state_dict'])\n\npth_load_time = time.time() - start_time\nprint(f\"   - Tiempo de carga: {pth_load_time:.1f} segundos ‚úÖ\")\n\nprint(\"\\nüìä Test 2: Cargar desde HuggingFace (sin cach√©)\")\n# Nota: Este tiempo ser√° m√°s r√°pido si HuggingFace tiene cach√©\n# En primera ejecuci√≥n sin cach√©: ~30-60 segundos\nprint(f\"   - Tiempo de descarga (medido antes): {download_time:.1f} segundos\")\n\nprint(\"\\n‚ö° Speedup:\")\nif download_time > pth_load_time:\n    speedup = download_time / pth_load_time\n    improvement = download_time - pth_load_time\n    print(f\"   - .pth es {speedup:.1f}x m√°s r√°pido\")\n    print(f\"   - Ahorra {improvement:.1f} segundos en startup\")\nelse:\n    print(f\"   - HuggingFace cach√© es m√°s r√°pido (ya estaba descargado)\")\n    print(f\"   - En primera ejecuci√≥n sin cach√©, .pth ser√° m√°s r√°pido\")\n\n# ============================================\n# 4. CREAR SCRIPT DE CARGA PARA PRODUCCI√ìN\n# ============================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"  CREANDO EJEMPLO DE CARGA PARA PRODUCCI√ìN\")\nprint(\"=\" * 70)\n\nproduction_example = \"\"\"\n# ====================================\n# PRODUCCI√ìN: Cargar CLIP desde .pth\n# ====================================\n\nimport torch\nfrom transformers import CLIPModel, CLIPProcessor\nfrom pathlib import Path\n\n# Rutas\nmodels_dir = Path('./models')\nclip_path = models_dir / 'clip_model.pth'\n\n# Cargar processor (r√°pido, ~1 segundo)\nclip_processor = CLIPProcessor.from_pretrained(\n    str(models_dir / 'clip_processor')\n)\n\n# Cargar modelo desde .pth (r√°pido, ~5-10 segundos)\nprint(\"Loading CLIP model...\")\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n\ncheckpoint = torch.load(clip_path, map_location='cpu')\nclip_model.load_state_dict(checkpoint['model_state_dict'])\nclip_model.eval()\n\nprint(f\"‚úÖ CLIP loaded from .pth in ~5-10 seconds\")\n\n# Usar para generar embeddings de nuevas im√°genes\nfrom PIL import Image\ndef encode_image(image_path):\n    image = Image.open(image_path).convert('RGB')\n    inputs = clip_processor(images=image, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        features = clip_model.get_image_features(**inputs)\n        features = features / features.norm(dim=-1, keepdim=True)\n\n    return features.cpu().numpy()\n\n# Ejemplo\nembedding = encode_image(\"new_product.jpg\")\nprint(f\"Embedding shape: {embedding.shape}\")  # (1, 512)\n\"\"\"\n\nexample_path = output_dir / \"load_clip_production.py\"\nwith open(example_path, 'w') as f:\n    f.write(production_example)\n\nprint(f\"‚úÖ Ejemplo guardado: {example_path}\")\n\n# ============================================\n# 5. GUARDAR METADATA\n# ============================================\n\nmetadata = {\n    'model_name': model_name,\n    'model_type': 'CLIP ViT-B/32',\n    'embedding_dim': 512,\n    'image_size': 224,\n    'file_size_mb': file_size_mb,\n    'download_time_seconds': download_time,\n    'pth_load_time_seconds': pth_load_time,\n    'speedup': download_time / pth_load_time if pth_load_time > 0 else 0,\n    'total_params': sum(p.numel() for p in clip_model.parameters()),\n    'saved_at': time.strftime('%Y-%m-%d %H:%M:%S'),\n    'usage': 'Load with: CLIPModel.from_pretrained() + load_state_dict()'\n}\n\nmetadata_path = output_dir / \"clip_model_info.json\"\nwith open(metadata_path, 'w') as f:\n    json.dump(metadata, f, indent=2)\n\nprint(f\"‚úÖ Metadata guardada: {metadata_path}\")\n\n# ============================================\n# RESUMEN FINAL\n# ============================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"  MODELO CLIP GUARDADO EXITOSAMENTE\")\nprint(\"=\" * 70)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-27T01:19:28.493121Z","iopub.execute_input":"2025-11-27T01:19:28.493366Z","iopub.status.idle":"2025-11-27T01:20:17.082321Z","shell.execute_reply.started":"2025-11-27T01:19:28.493346Z","shell.execute_reply":"2025-11-27T01:20:17.081160Z"}},"outputs":[{"name":"stderr","text":"2025-11-27 01:19:43.399059: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764206383.613097      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764206383.671522      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"======================================================================\n  RONDA 3B - GUARDAR MODELO CLIP COMO .PTH\n  Optimizaci√≥n para producci√≥n\n======================================================================\n\nüñ•Ô∏è Entorno: Kaggle\nüéÆ Device: cpu\n\nüìÇ Output: /kaggle/working\n\n======================================================================\n[1/3] CARGANDO CLIP DESDE HUGGINGFACE\n======================================================================\n\nüì• Descargando openai/clip-vit-base-patch32...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11b2086ea2aa4845b50bf73bed548233"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"789d5d9ce858454aa54ec98b45d3d7a1"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"453c4b873ce9493ab3a28907d288c74f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d0bd66462b84ee290514bc73564c8c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc34d38d14d74d009b1930ece30b8d3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20d69faf2c454dddafc35d9a7c3fb8f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a2f1bf22dde4880ae433dd629596b2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ccfeb6d91ef40be8911e5e490acafa9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3f7ce2061e745f6bc39b88440a76e05"}},"metadata":{}},{"name":"stdout","text":"‚úÖ CLIP descargado en 9.5 segundos\n   - Modelo: ViT-B/32\n   - Par√°metros: 151,277,313\n\n[2/3] GUARDANDO MODELO CLIP COMO .PTH\n\nüíæ Guardando modelo...\n‚úÖ Modelo guardado: /kaggle/working/clip_model.pth\n   - Tama√±o: 577.2 MB\n   - Tiempo de guardado: 0.7 segundos\n‚úÖ Processor config guardado: /kaggle/working\n\n[3/3] BENCHMARK: .PTH VS HUGGINGFACE\n\nüìä Test 1: Cargar desde .pth\n   - Tiempo de carga: 2.4 segundos ‚úÖ\n\nüìä Test 2: Cargar desde HuggingFace (sin cach√©)\n   - Tiempo de descarga (medido antes): 9.5 segundos\n\n‚ö° Speedup:\n   - .pth es 3.9x m√°s r√°pido\n   - Ahorra 7.1 segundos en startup\n\n======================================================================\n  CREANDO EJEMPLO DE CARGA PARA PRODUCCI√ìN\n======================================================================\n‚úÖ Ejemplo guardado: /kaggle/working/load_clip_production.py\n‚úÖ Metadata guardada: /kaggle/working/clip_model_info.json\n\n======================================================================\n  MODELO CLIP GUARDADO EXITOSAMENTE\n======================================================================\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"print(f\"\\nüì¶ Archivos generados:\")\nprint(f\"   - {clip_path} ({file_size_mb:.1f} MB)\")\nprint(f\"   - {output_dir / 'clip_processor/'} (config)\")\nprint(f\"   - {example_path} (ejemplo de uso)\")\nprint(f\"   - {metadata_path}\")\n\nprint(f\"\\n‚ö° Performance:\")\nprint(f\"   - Carga desde .pth: ~{pth_load_time:.1f}s\")\nprint(f\"   - Carga desde HuggingFace: ~{download_time:.1f}s\")\nprint(f\"   - Speedup: {download_time/pth_load_time:.1f}x\")\n\nprint(f\"\\nüí° Recomendaci√≥n para producci√≥n:\")\nprint(f\"   ‚úÖ Usar .pth para:\")\nprint(f\"      - Startup m√°s r√°pido ({pth_load_time:.1f}s vs {download_time:.1f}s)\")\nprint(f\"      - Sin dependencia de internet\")\nprint(f\"      - Sin riesgo de downtime de HuggingFace\")\nprint(f\"   ‚ö†Ô∏è Tradeoff:\")\nprint(f\"      - +{file_size_mb:.0f} MB de espacio en disco\")\n\nprint(f\"\\nüìÅ Para usar en tu API:\")\nprint(f\"   1. Descarga {clip_path.name}\")\nprint(f\"   2. Descarga carpeta clip_processor/\")\nprint(f\"   3. Usa c√≥digo en {example_path.name}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"  ‚úÖ LISTO PARA PRODUCCI√ìN\")\nprint(\"=\" * 70)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T01:21:55.551884Z","iopub.execute_input":"2025-11-27T01:21:55.552193Z","iopub.status.idle":"2025-11-27T01:21:55.560419Z","shell.execute_reply.started":"2025-11-27T01:21:55.552169Z","shell.execute_reply":"2025-11-27T01:21:55.559482Z"}},"outputs":[{"name":"stdout","text":"\nüì¶ Archivos generados:\n   - /kaggle/working/clip_model.pth (577.2 MB)\n   - /kaggle/working/clip_processor (config)\n   - /kaggle/working/load_clip_production.py (ejemplo de uso)\n   - /kaggle/working/clip_model_info.json\n\n‚ö° Performance:\n   - Carga desde .pth: ~2.4s\n   - Carga desde HuggingFace: ~9.5s\n   - Speedup: 3.9x\n\nüí° Recomendaci√≥n para producci√≥n:\n   ‚úÖ Usar .pth para:\n      - Startup m√°s r√°pido (2.4s vs 9.5s)\n      - Sin dependencia de internet\n      - Sin riesgo de downtime de HuggingFace\n   ‚ö†Ô∏è Tradeoff:\n      - +577 MB de espacio en disco\n\nüìÅ Para usar en tu API:\n   1. Descarga clip_model.pth\n   2. Descarga carpeta clip_processor/\n   3. Usa c√≥digo en load_clip_production.py\n\n======================================================================\n  ‚úÖ LISTO PARA PRODUCCI√ìN\n======================================================================\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}